# 对标方案比较分析

## 一、双目视觉 vs 单目深度估计（Depth-Anything）

| 对比项 | 文献方案：双目3D物体朝向检测 | 改进方案：单目深度估计（Depth-Anything） |
|--------|--------------------------------|------------------------------------------|
| 方法特征 | 依赖双摄像头，实现三维重建 | 使用单目图像预测深度图 |
| 主要模型 | 基于传统SLAM / 双目匹配 | Depth-Anything（MiDaS + 伪标签 + DINOv2） |
| 硬件依赖 | 双摄像头 + 高精度标定 | 普通单目摄像头即可 |
| 计算复杂度 | 高，存在同步与匹配消耗 | 中等，支持多尺度高效模型 |
| 可用场景 | 受限于标定和摄像头布置 | 适用于各种单摄场景（头盔、车前等） |
| 成本与可部署性 | 成本高，难以通用化 | 成本低，易于部署与扩展 |

## 二、纯传感器方向感知 vs 视觉感知替代方案（Mask R-CNN + Depth-Anything）

| 对比项 | 文献方案：纯传感器融合方向感知 | 改进方案：Mask R-CNN + 单目深度估计 |
|--------|------------------------------|-------------------------------------|
| 方法特征 | IMU + GPS + 磁力计等传感器融合 | 使用目标实例分割 + 深度图重建方向信息 |
| 硬件需求 | 增加方向传感器、改造车控平台 | 普通RGB摄像头 |
| 成本表现 | 高（需要专用传感器模组） | 极低（利用现有摄像头） |
| 泛化能力 | 差，传感器需专门调参 | 强，适用于多类目标与背景 |
| 计算效率 | 高效（嵌入式实现） | 中等（可裁剪模型实现轻量化） |
| 表现方式 | 融合多源传感器输出方向 | 提取目标3D位置 + 姿态估计 |

## 三、VLLM / Orient-Anything vs Mask R-CNN 实例分割

| 对比项 | 文献方案：VLLM / Orient-Anything | 改进方案：Mask R-CNN |
|--------|-------------------------------|-----------------------|
| 任务目标 | 实例级分割 / 朝向估计 | 实例级分割 + 掩码预测 |
| 模型复杂度 | 高，VLLM运行延迟 > 3000ms | 中，结构稳定可裁剪 |
| 输入形式 | 多模态 / 多步骤预处理 | 单图像输入，端到端输出 |
| 部署难度 | 高，需适配大模型推理框架 | 中，支持轻量化部署（MobileNet等） |
| 表现优势 | 开放世界能力强 | 精度高、结构清晰，广泛应用 |
