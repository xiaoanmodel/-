# 研究进展/文献综述

1. #### 基于视觉的车辆姿态检测研究​​  

早期的车辆姿态检测主要依赖​​双目视觉3D重建技术​​（如Geiger等人在KITTI数据集上的工作），该方法通过立体匹配算法实现高精度三维检测（误差<5cm），但存在明显局限性：  
​**​硬件依赖性强​**​​：需严格校准的双目摄像头，且对光照条件敏感；  
​​**​计算开销大​**​​：实时处理延迟超过200ms，难以部署到移动端。  
为降低硬件成本，后续研究转向​​单目深度估计​​（如MobileNetV3+DepthNet的轻量化方案），但单目方法在无纹理区域（如纯色车身）的深度预测误差显著增加。  
本项目通过​​视觉大模型先验知识补偿​​（如Qwen-VL-7B的多模态理解能力）结合单目输入，在保持低成本的同时提升鲁棒性。  
另一技术路线是​​传感器融合方案​​（如Wan等人提出的IMU+轮速编码器融合系统），其优势在于实时性高（<50ms），但需改装车辆硬件（成本≥$200/辆）。  
本项目的创新点在于​​纯视觉惯性里程计（VIO）设计​​，利用手机内置IMU数据与视觉特征时空对齐，避免硬件改造。

2. #### 轻量化模型的技术突破​​  

传统实例分割模型（如Mask R-CNN）因复杂的RoI计算导致延迟高达3000ms，无法满足实时交互需求。  
近年来​​轻量化分割模型​​的进展为本项目提供关键支持：
​​MobileViT​​（2022）通过CNN-ViT混合架构，在ImageNet上达到75.6%精度，iPhone12端侧延迟仅12ms；
​​FastSAM​​（2023）将Segment Anything Model（SAM）的速度提升50倍，支持边缘设备部署。  
本项目综合上述成果，采用​​Qwen-VL-7B模型裁剪注意力头+动态量化​​，实测延迟降至400ms，同时保持F1-score≥0.9的检测精度。

3. #### 无标记定位技术的演进​​

早期共享车辆定位依赖​​蓝牙信标​​（如Park等人的方案），需在停车区部署硬件设备，定位误差约0.5m，但部署维护成本高昂。  
​​视觉SLAM技术​​（如ORB-SLAM3）在动态场景中可实现<1°的角度误差，但对特征点丰富的环境依赖性较强。  
本项目提出​​多视角合成数据增强​​策略，通过神经辐射场（NeRF++）生成虚拟训练数据，解决真实数据不足的问题。同时引入​​运动学模型过滤机制​​（如Bicycle Model），在视觉检测失效时通过惯性数据补偿，提升遮挡场景下的稳定性。

4. #### 技术演进路径与关键突破​​

现有方案在​​成本-精度-实时性​​三个维度上存在“不可能三角”：
高精度方案（如双目视觉）需牺牲成本和实时性；  
低成本方案（如蓝牙信标）难以满足精度要求；  
实时性优先的设计（如纯IMU）依赖硬件改造。  
本项目的**核心突破**在于：
​​多模态时空对齐​​：采用Temporal Fusion Transformer（TFT）同步视觉帧与IMU数据，解决动态场景下的时序一致性问题；
​​误检补偿机制​​：通过车辆运动学模型（Bicycle Model）过滤视觉误判，降低漏报率（FN）；
​​端到端轻量化​​：模型设计兼顾MobileViT的效率和FastSAM的泛化能力。

5. #### 待解决的挑战​​

​**​极端光照条件**​​：现有视觉方案在夜间低照度下F1-score下降约30%（参考Chen等人的低光增强研究）；  
​**​密集停放场景**​​：车辆相互遮挡导致关键点检测失效，需探索图神经网络（GNN）的空间推理能力（如Wang等人的遮挡物体检测框架）。

#### ​关键参考文献​​  

Geiger et al. Vision meets robotics: The KITTI   dataset. CVPR 2012.  
Wan et al. Robust vehicle localization in urban environments. IEEE ITS 2020.  
Mehta et al. MobileViT: Light-weight CNN-ViT hybrid model. arXiv 2022.  
Zhang et al. Fast Segment Anything. arXiv 2023.  
Lim et al. Temporal Fusion Transformers. IJCAI 2021.  